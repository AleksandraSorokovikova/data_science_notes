# Содержание:

1. [Обработка данных](#Обработка-данных)
    1. [Предобработка](#Предобработка)
    2. [Категоризация](#Категоризация)
    3. [Устранение дубликатов](#Устранение-дубликатов)
    4. [Стемминг](#Стемминг)
    5. [Лемматизация](#Лемматизация)
    6. [Заполнение пропусков](#Заполнение-пропусков)
    7. [Удаление выбросов](#Удаление-выбросов)
    8. [Разбиение выборки](#Разбиение-выборки)
    9. [Приведение категориальных признаков к численным](#Приведение-категориальных-признаков-к-численным)
    10. [Масштабирование количественных признаков](#Масштабирование-количественных-признаков)
    11. [Снижение размерности или метод главных компонент PCA](#Снижение-размерности-или-метод-главных-компонент-PCA)
    12. [Увеличение и уменьшение выборки upsampling и downsampling](#Увеличение-и-уменьшение-выборки-upsampling-и-downsampling)
    13. [Bootstrapping](#Bootstrapping)
2. [Метрики](#Метрики)
    1. [Accuracy](#Accuracy)
    2. [Precision](#Precision)
    3. [Recall](#Recall)
    4. [F1-мера](#F1-мера)
    5. [AUC-ROC](#AUC-ROC)
    6. [MSE](#MSE)
    7. [RMSE](#RMSE)
    8. [MAE](#MAE)
    9. [R2](#R2)
3. [Модели](#Модели)
    1. [K-ближайших соседей](#K-ближайших-соседей)
    2. [Наивный Байес](#Наивный-Байес)
    3. [Линейная регрессия](#Линейная-регрессия)
    4. [Логистическая регрессия](#Логистическая-регрессия)
    5. [Градиентный спуск](#Градиентный-спуск)
    6. [Гребневая регрессия](#Гребневая-регрессия)
    7. [Стохастический спуск](#Стохастический-спуск)
    8. TODO

TODO


# Обработка данных

### Предобработка:
- Перевод строковых значений в числа: метод в **pandas** `to_numeric()`
- Перевод строки в дату и время: метод в **pandas** `to_datetime()`
- Приведение все к одному регистру: строковый метод `lower()`

### Категоризация
*Зачем*: в наборе данных некоторые категориальные значения (например, возраст) могут быть представлены только единожды, поэтому объединение их в группы облегчит дальнейшее изучение. 

*Вспомогательные методы:* метод в **pandas** `apply()` и `group()`

### Устранение дубликатов
*Зачем*: дубликаты в наблюдениях могут повлиять на интерпретацию результатов исследования, поэтому их можно удалять. 

*Вспомогательные методы:* метод в **pandas** `duplicated()` совместно с `sum()`, `value_counts()`

### Стемминг
*Зачем*: нужен при разделении строк по категориям. Стемминг находит *основу* слова (чтобы формы слова не повлияли на разделение по разным категориям)

*Вспомогательные методы:* метод в **nltk.stem SnowballStemmer** `stem`

### Лемматизация
*Зачем*: затем же, зачем и стемминг. Лемматизация приводит слово к *словарной форме*(например, существительное к именительному падежу, глаголу к инфинитиву).

*Вспомогательные методы:* метод в **pymystem3 Mystem** `lemmatize`, для подсчета количества вхождений **collections Counter**

### Заполнение пропусков
*Зачем*: поможет избежать большой потери данных, если пропусков слишком много, и улучшить качество модели. 

- заполнение пропусков по умолчанию. *Вспомогательные методы:* метод в **pandas** `fillna()`
- заполнение количественных признаков характерными значениями: медиана (`median()`), среднее арифметическое (`mean()`) и тд.
- посмотреть на распределение данных

Если заполнение пропусков недопустимо, можно сбросить столбцы или строки с пропущенными значениями: метод в **pandas** `dropna()`

### Удаление выбросов
*Зачем*: в данных могут наблюдаться определенные закономерности (их можно увидеть с помощью ститистик и графиков), поэтому если в таких данных встречаются выбросы (например, указан 3014 год или у человека 40 детей), их необходимо удалять, чтобы они не влияли обучение.

*Вспомогательные методы*: чтобы удалить выбросы, их сначала необходимо заметить. Для этого в следующих разделах будет показаны инструменты (построение графиков, вычисление статистик).

### Разбиение выборки
*Зачем*: чтобы лучше оценивать работоспособность модели, необходимо тестировать ее на данных, которые она раньше не видела.

*Вспомогательные методы*: метод в **sklearn.model_selection** `train_test_split()`

### Приведение категориальных признаков к численным
*Зачем*: некоторые модели не работают с категориальными признаками (например, строками), для этого их необходимо преобразовать в численные.

Методы:

- ***Прямое кодирование (One-Hot-Encoding)***: под каждое значение категориального признака создается новый столбец, в котором ставится 1, если у наблюдения есть данный признак, 0 в противном случае. *Вспомогательные методы*: метод в **pandas** `get_dummies()`. Чтобы избежать *дамми-ловушки* (когда после добавления таких столбцов один из них становится лишним, так как в остальных и так достаточно информации; например, категория пол: для одного пола создается столбец принадлежности к данному полу, соответственно, столбец, где указана принадлежность к другому полу, не нужен, так как можно в первом указать, что данный человек не принадлежит первому полу). Для этого в методе `get_dummies()` используется аргумент `drop_first_True`
- ***Порядковое кодирование (Ordinal-Encoding)***: присваивает каждому значению категориального признака номер и записывает их в новый столбец (например, в столбце "Город" есть три значения: Москва, Санкт-Петербург, Иркутск. Тогда после преобразования во всех строках, где город был "Москва", теперь будет стоять 0, где был "Санкт-Петербург", будет стоять 1 и так далее). *Вспомогательные методы*: метод в **sklearn.preprocessing OrdinalEncoder** `fit()` - для обучения на данных, `transform()` - для преобразования.

### Масштабирование количественных признаков
*Зачем*: модели могут быть чувствительны к разбросам значений (например, если мы поменяем размерность роста с сантиметров на дюймы), поэтому нужно их масштабировать. Для этого их можно стандартизировать (то есть привести среднее признака к 0, а дисперсию к 1). 

*Вспомогательные методы*: метод в **sklearn.preprocessing StandardScaler** `fit()` - для обучения и `transform()` - для преобразования.

### Снижение размерности или метод главных компонент PCA
*Зачем*: если в данных есть признаки, которые сильно коррелируют друг с другом, и нам необходимо снизить размерность, то есть слить эти признаки в один, можно применить PCA. Модель может переобучаться из-за переизбытка признаков, особенно если они сильно друг от друга зависят. В данном случае PCA может улучшить модель, а также лучше проиллюстрировать взаимосвязь в данных.

*Вспомогательные методы*: метод в **sklearn.decomposition PCA** `fit_transform()` - это `fit()` и `transform()` в одном.

### Увеличение и уменьшение выборки upsampling и downsampling
*Зачем*: классы в данных могут быть не сбалансированы (например, в целевом признаке 90% будет занимать класс 0, и только остальные 10% класс 1). Поэтому можно увеличить количество наблюдений, в которых целевой признак находится в меньшинстве (в примере это класс 1), или наоборот уменьшить количество наблюдений, чей целевой признак находится в большинстве (класс 0). 

Для этого нужно разбить выборку (не тестовую) на классы, и случайным образом либо удалить наблюдения доминирующего класса, либо скопировать какое-то количество наблюдений дефицитного класса, и объединить с выборкой, после этого перемешать.

*Вспомогательные методы*: метод в **pandas** `concat` для объединение выборок, `shuffle` - для перемешивания наблюдений

### Bootstrapping
*Зачем*: иногда может возникать такая ситуация, когда данные в выборке распределены так: около половины значений близки к 200, а половина к 0, тогда медиана может быть не очень показательной. Если бы имелась возможность неоднократно генерировать новые выборки, можно бы было вычислить медиану каждой и обратиться к распределению этих медиан. Если такой возможности нет, то можно обратиться к bootstrapping или методу рамножение выборок, когда из имеющихся выборок генерируют новые случайные выборки с возвратом размером n точек, а затем измеряют медианы этих выборок.

https://github.com/joelgrus/data-science-from-scratch/blob/master/scratch/multiple_regression.py

TODO: вставить ссылку получше


# Метрики

### Accuracy
Измеряет долю правильных ответов, то есть $$accuracy = \frac{N_{true}}{N_{all}}$$ где $N_{true}$ - количество правильных ответов, $N_{all}$ - количество всех ответов.

***Референтные значения***: чем ближе метрика к 1, тем лучше модель; чем ближе метрика к 0, тем модель хуже.

***Где применяется***: задача *классификации*. Однако если целевые классы несбалансированы, **accuracy** может быть завышена.

***Вспомогательные методы***: метод в **sklearn.metrics** `accuracy()`

### Precision
Определяет, как много отрицательных ответов нашла модель, пока искала положительные. Чем больше отрицательных, тем ниже точность.
$$precision = \frac{TP}{TP + FP}$$
где $TP$ - истинноположительные ответы, $FP$ - ложноположительные ответы. 

***Референтные значения***: чем ближе метрика к 1, тем лучше модель; чем ближе метрика к 0, тем модель хуже.

***Где применяется***: задача *классификации*

***Вспомогательные методы***: метод в **sklearn.metrics** `precision_score()`

### Recall
Определяет какую долю положительных среди всех ответов выделила модель. Если значение **recall** близко к единице, то модель хорошо ищет положительные объекты.

$$recall = \frac{TP}{TP+FN}$$
где $TP$ - истинноположительные ответы, $FN$ - ложноотрицательные ответы. 

***Референтные значения***: чем ближе метрика к 1, тем лучше модель; чем ближе метрика к 0, тем модель хуже.

***Где применяется***: задача *классификации*

***Вспомогательные методы***: метод в **sklearn.metrics** `recall_score()`

### F1-мера
Данная метрика объединяет **precision** и **recall**, она является гароническим средним между ними:
$$f1 = \frac{2*precision*recall}{precision+recall}$$

***Референтные значения***: чем ближе метрика к 1, тем лучше модель; чем ближе метрика к 0, тем модель хуже.

***Где применяется***: задача *классификации*

***Вспомогательные методы***: метод в **sklearn.metrics** `f1_score`

### AUC-ROC
**ROC-кривая** показывается соотношение *TPR* и *FPR* - доля истинноположительных ответов и доля ложноположительных ответов. Чем выше данная кривая, тем лучше модель. ROC-кривая для случайной модели можно представить в виде прямой $y=x$. Соответственно, если кривая модели выше случайной, то она предсказывают лучше, чем случайная. **AUC-ROC** (area under curve roc) - это площадь под ROC-кривой.

***Референтные значения***: чем ближе метрика к 1, тем лучше модель; чем ближе метрика к 0, тем модель хуже. 

***Где применяется***: задача *классификации*

***Вспомогательные методы***: метод в **sklearn.metrics** `roc_auc_score`

В отличие от других метрик, данная метрика принимает в качестве аргумента вероятности класса 1, а не предсказаения модели.

### MSE
Определяет среднеквадратичную ошибку (mean squared error).
$$MSE = \frac{1}{N}\sum^{N}_{i=1}(y_i-Y_i)^2$$
где $N$ - количество наблюдений, $y_i$ - предсказание модели, $Y_i$ - правильный ответ

***Референтные значения***: чем меньше метрика, тем лучше

***Где применяется***: задача *регрессии*

***Вспомогательные методы***: метод в **sklearn.metrics** `mean_squared_error`

### RMSE
Определяет корень от среднеквадратичной ошибки.
$$RMSE = \sqrt{MSE}$$

***Референтные значения***: чем меньше метрика, тем лучше

***Где применяется***: задача *регрессии*

### MAE
Определяет среднее абсолютное расстояние между прогнозируемыми и целевыми значениями. Данная метрика более устойчива к выбросам, чем **MSE**, а также является менее чувствительной к большим значениям, нежели **RMSE** 

$$MSE = \frac{1}{N}\sum^{N}_{i=1}|y_i-Y_i|$$
где $N$ - количество наблюдений, $y_i$ - предсказание модели, $Y_i$ - правильный ответ

***Референтные значения***: чем меньше метрика, тем лучше

***Где применяется***: задача *регрессии*

***Вспомогательные методы***: метод в **sklearn.metrics** `mean_absolute_error`


### R2

Коэффициент детерминации. Измеряет долю суммарной вариации в зависимой переменной. 

$$R2 = 1 - \frac{MSE_{model}}{MSE_{mean}}$$
где $MSE_{model}$ - MSE модели, $MSE_{mean}$ - MSE среднего

***Референтные значения***: Чем ближе значение $R2$ к единице, тем лучше предсказывает модель. Если $R2 = 0$, то модель работает так же, как случайная. Если $R2<0$, то модель предсказывает хуже, чем случайная.

***Где применяется***: задача *регрессии*

***Вспомогательные методы***: метод в **sklearn.metrics** `r2_score`


# Модели

## K-ближайших соседей

Данная модель является простейшей. Для ее использования необходимо:

- некоторое представление о расстоянии
- предположение, что точки, близко расположенные друг к другу, подобны

Данные должны быть представлены в виде векторов, где каждое значение является числом. Алгоритм следующий:

- Вычислить расстояние до каждого из объектов обучающей выборки
- Отобрать k объектов обучающей выборки, расстояние до которых минимально
- Класс классифицируемого объекта — это класс, наиболее часто встречающийся среди k ближайших соседей

***Где применяется***: задача *классификации* и *регрессии*

***Вспомогательные методы***: https://scikit-learn.org/stable/modules/neighbors

## Наивный Байес

##### Теорема Байеса

События $A_1, ..., A_k$ образуют полную группу (то есть $P(A_1) + ... + P(A_k) = 1$).

Тогда $P(B)$ - полная вероятность события $B$:
$$P(B) = P(A_1)*P(B|A_1) + ... + P(A_k)*P(B|A_k)$$

Вероятность наступления события $A_i$ при условии, что $B$ произошла **по теореме Байеса** равна:
$$P(A_i|B) = \frac{P(B|A_i)*P(A_i)}{P(B)}$$

##### Сглаживание

Например, если мы создаем спам-фильтр, и статистика указывает на то, что слово *A* содержится только в неспамных письмах, то при применении данной теоремы для фильтрации писем все сообщения, которые содержат данное слово, будут являться неспамными (что может быть неверное), так как $P(A|S) = 0$.

Чтобы это избежать, мы можем подобрать такую константу $k$, чтобы *сгладить* подобные выборсы:
$$P(X_i|S) = \frac{k+N_{x_i}}{2k+N}$$
где $N_{x_i}$ - число спамных сообщений, содержащих слово $x_i$, $N$ - число спамных сообщений


##### Применение

С помощью данной модели мы можем *классифицировать* признаки на разные группы, вычислив вероятность их попадания в тот или иной класс.


***Где применяется***: задача *классификации* 

***Вспомогательные методы***: https://scikit-learn.org/stable/modules/naive_bayes


## Линейная регрессия

Данная модель применяется, если есть основания полагать, что между признаками и целевым признаком существует линейная связь (например, подсчитав корреляцию).

Если представить, что признак, который мы ищем ($y_i$), линейно зависит от данного признака (x_i), то мы можем найти такие коэффициенты $\alpha$ и $\beta$, что:
$$y_i = \alpha * x_i + \beta + \epsilon$$
где $\epsilon$ какая-то незначительная ошибка. Тогда найдя коэффициенты при обучении модели, можем спронозировать ответы для будущих признаков. Если есть несколько независимых признаков, по которым нужно предсказать целевой признак, то можем применить множественную линейную регрессию, то есть необходимо найти такие коэффициенты $\beta_1, ..., \beta_n$, что 
$$y_i = \alpha + \beta_1*x_1 + ... \beta_n*x_n + \epsilon$$
где $(x_1, ..., x_n)$ - вектор признаков.

Чтобы применить модель множественной линеной регрессии, вектора признаков должны быть ЛНЗ.


***Где применяется***: задача *регрессии*

***Вспомогательные методы***: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html

## Логистическая регрессия

## Градиентный спуск

## Гребневая регрессия

## Стохастический спуск

## TODO (еще много моделей)

# Виды графиков
+графики для метрик

# Данные для исследования (и что они показывают)
корреляция, медиана, дисперсия, гипотезы и тд

# Сбор данных
